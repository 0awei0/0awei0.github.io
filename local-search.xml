<?xml version="1.0" encoding="utf-8"?>
<search>
  
  
  
  <entry>
    <title>垃圾分类小demo</title>
    <link href="/2022/07/19/%E5%9E%83%E5%9C%BE%E5%88%86%E7%B1%BB%E5%B0%8Fdemo/"/>
    <url>/2022/07/19/%E5%9E%83%E5%9C%BE%E5%88%86%E7%B1%BB%E5%B0%8Fdemo/</url>
    
    <content type="html"><![CDATA[<h2 id="1-环境搭建"><a href="#1-环境搭建" class="headerlink" title="1. 环境搭建"></a>1. 环境搭建</h2><p>使用的环境为pytorch1.11，直接去官网即可下载<a href="https://pytorch.org/">PyTorch</a>，去官网选择你需要的配置，直接复制命令在命令行进行粘贴即可：</p><p><img src="1.png" alt="image-20220719092532904"></p><h2 id="2-数据准备"><a href="#2-数据准备" class="headerlink" title="2. 数据准备"></a>2. 数据准备</h2><h3 id="2-1-数据格式"><a href="#2-1-数据格式" class="headerlink" title="2.1 数据格式"></a>2.1 数据格式</h3><p><img src="2.png" alt="image-20220719092638421"></p><p>数据集中共有厨余垃圾，可回收物，其他垃圾，有害垃圾四种垃圾，每种垃圾各有11种，一共有44个小类，代码放在code文件夹中，与数据集放在同一文件目录下。</p><p><img src="3.png" alt="image-20220719092833905"></p><h3 id="2-2-划分数据集"><a href="#2-2-划分数据集" class="headerlink" title="2.2 划分数据集"></a>2.2 划分数据集</h3><p>找到了数据集之后我们需要把数据集划分为训练集和测试集，建立dataset.py文件，具体代码如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> os<br><span class="hljs-keyword">from</span> shutil <span class="hljs-keyword">import</span> copy<br><span class="hljs-keyword">import</span> random<br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">mkfile</span>(<span class="hljs-params">file</span>):<br>    <span class="hljs-keyword">if</span> <span class="hljs-keyword">not</span> os.path.exists(file):<br>        os.makedirs(file)<br><br><span class="hljs-comment">#你自己的数据集路径</span><br>file_path = <span class="hljs-string">&quot;../数据集//&quot;</span><br>garbage_class = os.listdir(<span class="hljs-string">&quot;../数据集&quot;</span>)<br><br><span class="hljs-comment"># 创建 训练集train 文件夹，并由5种类名在其目录下创建5个子目录</span><br>mkfile(<span class="hljs-string">&#x27;../data/train&#x27;</span>)<br><span class="hljs-keyword">for</span> cla <span class="hljs-keyword">in</span> garbage_class:<br>    mkfile(<span class="hljs-string">&#x27;../data/train/&#x27;</span> + cla)<br><br><span class="hljs-comment"># 创建 验证集val 文件夹，并由5种类名在其目录下创建5个子目录</span><br>mkfile(<span class="hljs-string">&#x27;../data/val&#x27;</span>)<br><span class="hljs-keyword">for</span> cla <span class="hljs-keyword">in</span> garbage_class:<br>    mkfile(<span class="hljs-string">&#x27;../data/val/&#x27;</span> + cla)<br><br><span class="hljs-comment"># 划分比例，训练集 : 验证集 = 9 : 1</span><br>split_rate = <span class="hljs-number">0.1</span><br><br><span class="hljs-comment"># 遍历5种花的全部图像并按比例分成训练集和验证集</span><br><span class="hljs-keyword">for</span> cla <span class="hljs-keyword">in</span> garbage_class:<br>    cla_path = file_path + <span class="hljs-string">&#x27;/&#x27;</span> + cla + <span class="hljs-string">&#x27;/&#x27;</span>  <span class="hljs-comment"># 某一类别花的子目录</span><br>    images = os.listdir(cla_path)  <span class="hljs-comment"># iamges 列表存储了该目录下所有图像的名称</span><br>    num = <span class="hljs-built_in">len</span>(images)<br>    eval_index = random.sample(images, k=<span class="hljs-built_in">int</span>(num * split_rate))  <span class="hljs-comment"># 从images列表中随机抽取 k 个图像名称</span><br>    <span class="hljs-keyword">for</span> index, image <span class="hljs-keyword">in</span> <span class="hljs-built_in">enumerate</span>(images):<br>        <span class="hljs-comment"># eval_index 中保存验证集val的图像名称</span><br>        <span class="hljs-keyword">if</span> image <span class="hljs-keyword">in</span> eval_index:<br>            image_path = cla_path + image<br>            new_path = <span class="hljs-string">&#x27;../data/val/&#x27;</span> + cla<br>            copy(image_path, new_path)  <span class="hljs-comment"># 将选中的图像复制到新路径</span><br><br>        <span class="hljs-comment"># 其余的图像保存在训练集train中</span><br>        <span class="hljs-keyword">else</span>:<br>            image_path = cla_path + image<br>            new_path = <span class="hljs-string">&#x27;../data/train/&#x27;</span> + cla<br>            copy(image_path, new_path)<br>        <span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;\r[&#123;&#125;] processing [&#123;&#125;/&#123;&#125;]&quot;</span>.<span class="hljs-built_in">format</span>(cla, index + <span class="hljs-number">1</span>, num), end=<span class="hljs-string">&quot;&quot;</span>)  <span class="hljs-comment"># processing bar</span><br>    <span class="hljs-built_in">print</span>()<br><br><span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;processing done!&quot;</span>)<br></code></pre></td></tr></table></figure><p>划分好的数据集如下：</p><p><img src="4.png" alt="image-20220719093314655"></p><h2 id="3-模型训练"><a href="#3-模型训练" class="headerlink" title="3. 模型训练"></a>3. 模型训练</h2><h3 id="3-1-数据增强"><a href="#3-1-数据增强" class="headerlink" title="3.1 数据增强"></a>3.1 数据增强</h3><p>数据增强可以有效避免过拟合，使用torchvision中的transforms可以方便的对训练集数据进行数据增强，常见的数据增强方法有旋转，剪切等，本文采用简单的裁剪和翻转。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># 导入包</span><br><span class="hljs-keyword">import</span> torch<br><span class="hljs-keyword">import</span> torch.nn <span class="hljs-keyword">as</span> nn<br><span class="hljs-keyword">from</span> torchvision <span class="hljs-keyword">import</span> transforms, datasets<br><span class="hljs-keyword">import</span> torch.optim <span class="hljs-keyword">as</span> optim<br><span class="hljs-keyword">import</span> json<br><span class="hljs-keyword">import</span> time<br><br><span class="hljs-comment"># 使用GPU训练</span><br>device = torch.device(<span class="hljs-string">&quot;cuda&quot;</span> <span class="hljs-keyword">if</span> torch.cuda.is_available() <span class="hljs-keyword">else</span> <span class="hljs-string">&quot;cpu&quot;</span>)<br><span class="hljs-built_in">print</span>(device)<br><br>data_transform = &#123;<br>    <span class="hljs-string">&quot;train&quot;</span>: transforms.Compose([transforms.RandomResizedCrop(<span class="hljs-number">224</span>),  <span class="hljs-comment"># 随机裁剪，再缩放成 224×224</span><br>                                 transforms.RandomHorizontalFlip(p=<span class="hljs-number">0.5</span>),  <span class="hljs-comment"># 水平方向随机翻转，概率为 0.5, 即一半的概率翻转, 一半的概率不翻转</span><br>                                 transforms.RandomHorizontalFlip(<span class="hljs-number">0.5</span>),<br>                                 transforms.ToTensor(),<br>                                 transforms.Normalize((<span class="hljs-number">0.5</span>, <span class="hljs-number">0.5</span>, <span class="hljs-number">0.5</span>), (<span class="hljs-number">0.5</span>, <span class="hljs-number">0.5</span>, <span class="hljs-number">0.5</span>))]),<br><br>    <span class="hljs-string">&quot;val&quot;</span>: transforms.Compose([transforms.Resize((<span class="hljs-number">224</span>, <span class="hljs-number">224</span>)),  <span class="hljs-comment"># cannot 224, must (224, 224)</span><br>                               transforms.ToTensor(),<br>                               transforms.Normalize((<span class="hljs-number">0.5</span>, <span class="hljs-number">0.5</span>, <span class="hljs-number">0.5</span>), (<span class="hljs-number">0.5</span>, <span class="hljs-number">0.5</span>, <span class="hljs-number">0.5</span>))])&#125;<br></code></pre></td></tr></table></figure><h3 id="3-2-创建DataLoader"><a href="#3-2-创建DataLoader" class="headerlink" title="3.2 创建DataLoader"></a>3.2 创建DataLoader</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">from</span> torch.utils.data <span class="hljs-keyword">import</span> DataLoader<br><br><span class="hljs-comment"># 获取图像数据集的路径</span><br>image_path = <span class="hljs-string">&quot;../data/&quot;</span>  <span class="hljs-comment">#garbage data_set path</span><br><br><span class="hljs-comment"># 导入训练集并进行预处理</span><br>train_dataset = datasets.ImageFolder(root=image_path + <span class="hljs-string">&quot;/train&quot;</span>,<br>                                     transform=data_transform[<span class="hljs-string">&quot;train&quot;</span>])<br>train_num = <span class="hljs-built_in">len</span>(train_dataset)<br><br><span class="hljs-comment"># 按batch_size分批次加载训练集</span><br>train_loader = DataLoader(train_dataset,  <span class="hljs-comment"># 导入的训练集</span><br>                          batch_size=<span class="hljs-number">16</span>,  <span class="hljs-comment"># 每批训练的样本数</span><br>                          shuffle=<span class="hljs-literal">True</span>,  <span class="hljs-comment"># 是否打乱训练集</span><br>                          num_workers=<span class="hljs-number">0</span>)  <span class="hljs-comment"># 使用线程数</span><br>                          <br><span class="hljs-comment"># 导入验证集并进行预处理</span><br>validate_dataset = datasets.ImageFolder(root=image_path + <span class="hljs-string">&quot;/val&quot;</span>,<br>                                        transform=data_transform[<span class="hljs-string">&quot;val&quot;</span>])<br>val_num = <span class="hljs-built_in">len</span>(validate_dataset)<br><br><span class="hljs-comment"># 加载验证集</span><br>validate_loader = torch.utils.data.DataLoader(validate_dataset,  <span class="hljs-comment"># 导入的验证集</span><br>                                              batch_size=<span class="hljs-number">16</span>,<br>                                              shuffle=<span class="hljs-literal">True</span>,<br>                                              num_workers=<span class="hljs-number">0</span>)<br></code></pre></td></tr></table></figure><h3 id="3-3-对数据集标签进行映射"><a href="#3-3-对数据集标签进行映射" class="headerlink" title="3.3 对数据集标签进行映射"></a>3.3 对数据集标签进行映射</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><code class="hljs python">garbage_list = train_dataset.class_to_idx<br>class_dict = <span class="hljs-built_in">dict</span>((val, key) <span class="hljs-keyword">for</span> key, val <span class="hljs-keyword">in</span> garbage_list.items())<br><br><span class="hljs-comment"># 将 class_dict 写入 json 文件中</span><br>json_str = json.dumps(class_dict, indent=<span class="hljs-number">4</span>, ensure_ascii=<span class="hljs-literal">False</span>)<br><span class="hljs-keyword">with</span> <span class="hljs-built_in">open</span>(<span class="hljs-string">&#x27;class_indices.json&#x27;</span>, <span class="hljs-string">&#x27;w&#x27;</span>, encoding=<span class="hljs-string">&#x27;utf-8&#x27;</span>) <span class="hljs-keyword">as</span> json_file:<br>    json_file.write(json_str)<br></code></pre></td></tr></table></figure><p>生成的class_indices.json文件如下：</p><p><img src="5.png" alt="image-20220719093835515"></p><p>包含0~44个垃圾的类型索引，此文件主要用于后续推理时使用，对应出垃圾类别。</p><h3 id="3-4-模型选择"><a href="#3-4-模型选择" class="headerlink" title="3.4 模型选择"></a>3.4 模型选择</h3><p>本文选择在ImageNet上预训练的efficientnet_b1进行迁移学习，这个网络的大小只有30多MB，虽然效果还不错，但如果要追求更高的精度，且有大显存的GPU，可以考虑采用更大的网络（如efficientnet_b7）进行训练，可以得到更好的效果。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> torchvision<br><span class="hljs-keyword">from</span> tqdm <span class="hljs-keyword">import</span> tqdm<br><br>net = torchvision.models.efficientnet_b1(pretrained=<span class="hljs-literal">True</span>)<br>net.to(device)  <span class="hljs-comment"># 分配网络到指定的设备（GPU/CPU）训练</span><br>loss_function = nn.CrossEntropyLoss()  <span class="hljs-comment"># 交叉熵损失</span><br>optimizer = optim.Adam(net.parameters(), lr=<span class="hljs-number">0.0002</span>)  <span class="hljs-comment"># 优化器（训练参数，学习率）</span><br><br>save_path = <span class="hljs-string">&#x27;model.pth&#x27;</span><br>best_acc = <span class="hljs-number">0.0</span><br>test_accs = []<br><br><span class="hljs-keyword">for</span> epoch <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">10</span>):<br>    net.train()  <span class="hljs-comment"># 训练过程中开启 Dropout</span><br>    running_loss = <span class="hljs-number">0.0</span>  <span class="hljs-comment"># 每个 epoch 都会对 running_loss  清零</span><br>    time_start = time.perf_counter()  <span class="hljs-comment"># 对训练一个 epoch 计时</span><br><br>    step = <span class="hljs-number">0</span><br>    <span class="hljs-keyword">for</span> step, data <span class="hljs-keyword">in</span> <span class="hljs-built_in">enumerate</span>(train_loader, start=<span class="hljs-number">0</span>):  <span class="hljs-comment"># 遍历训练集，step从0开始计算</span><br>        images, labels = data  <span class="hljs-comment"># 获取训练集的图像和标签</span><br>        optimizer.zero_grad()  <span class="hljs-comment"># 清除历史梯度</span><br><br>        outputs = net(images.to(device))  <span class="hljs-comment"># 正向传播</span><br>        loss = loss_function(outputs, labels.to(device))  <span class="hljs-comment"># 计算损失</span><br>        loss.backward()  <span class="hljs-comment"># 反向传播</span><br>        optimizer.step()  <span class="hljs-comment"># 优化器更新参数</span><br>        running_loss += loss.item()<br><br>        <span class="hljs-comment"># 打印训练进度（使训练过程可视化）</span><br>        rate = (step + <span class="hljs-number">1</span>) / <span class="hljs-built_in">len</span>(train_loader)  <span class="hljs-comment"># 当前进度 = 当前step / 训练一轮epoch所需总step</span><br>        a = <span class="hljs-string">&quot;*&quot;</span> * <span class="hljs-built_in">int</span>(rate * <span class="hljs-number">50</span>)<br>        b = <span class="hljs-string">&quot;.&quot;</span> * <span class="hljs-built_in">int</span>((<span class="hljs-number">1</span> - rate) * <span class="hljs-number">50</span>)<br>        <span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;\rtrain loss: &#123;:^3.0f&#125;%[&#123;&#125;-&gt;&#123;&#125;]&#123;:.3f&#125;&quot;</span>.<span class="hljs-built_in">format</span>(<span class="hljs-built_in">int</span>(rate * <span class="hljs-number">100</span>), a, b, loss), end=<span class="hljs-string">&quot;&quot;</span>)<br><br>    <span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;%f s&#x27;</span> % (time.perf_counter() - time_start))<br><br>    <span class="hljs-comment">########################################### validate ###########################################</span><br>    net.<span class="hljs-built_in">eval</span>()  <span class="hljs-comment"># 验证过程中关闭 Dropout</span><br><br>    acc = <span class="hljs-number">0.0</span><br>    <span class="hljs-keyword">with</span> torch.no_grad():<br>        <span class="hljs-keyword">for</span> val_data <span class="hljs-keyword">in</span> tqdm(validate_loader):<br>            val_images, val_labels = val_data<br>            outputs = net(val_images.to(device))<br>            predict_y = torch.<span class="hljs-built_in">max</span>(outputs, dim=<span class="hljs-number">1</span>)[<span class="hljs-number">1</span>]  <span class="hljs-comment"># 以output中值最大位置对应的索引（标签）作为预测输出</span><br>            acc += (predict_y == val_labels.to(device)).<span class="hljs-built_in">sum</span>().item()<br>        val_accurate = acc / val_num<br>        test_accs.append(val_accurate)<br><br>        <span class="hljs-comment"># 保存准确率最高的那次网络参数</span><br>        <span class="hljs-keyword">if</span> val_accurate &gt; best_acc:<br>            best_acc = val_accurate<br>            torch.save(net.state_dict(), save_path)<br><br>        <span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;[epoch %d] train_loss: %.3f test_accuracy: %.3f \n&#x27;</span> %<br>              (epoch + <span class="hljs-number">1</span>, running_loss / step, val_accurate))<br><br><span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;Finished Training&#x27;</span>)<br></code></pre></td></tr></table></figure><h3 id="3-5-精度可视化"><a href="#3-5-精度可视化" class="headerlink" title="3.5 精度可视化"></a>3.5 精度可视化</h3><p>部分训练结果如下</p><p><img src="7.png" alt="image-20220719143216553"></p><p>作出测试集acc曲线：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> matplotlib.pyplot <span class="hljs-keyword">as</span> plt<br><br>plt.ylim(<span class="hljs-number">0.9</span>, <span class="hljs-number">1</span>)<br>x = <span class="hljs-built_in">list</span>(<span class="hljs-built_in">range</span>(<span class="hljs-number">1</span>, <span class="hljs-number">11</span>))<br><br>plt.plot(x, test_accs, label=<span class="hljs-string">&quot;test set acc&quot;</span>)<br><br>plt.legend()  <span class="hljs-comment"># 显示图例</span><br>plt.xticks(x)<br>plt.xlabel(<span class="hljs-string">&quot;epoch&quot;</span>)  <span class="hljs-comment"># X轴标签</span><br>plt.ylabel(<span class="hljs-string">&quot;%acc&quot;</span>)  <span class="hljs-comment"># Y轴标签</span><br>plt.title(<span class="hljs-string">&#x27;acc&#x27;</span>)<br>plt.savefig(<span class="hljs-string">&quot;acc.jpg&quot;</span>)<br></code></pre></td></tr></table></figure><p><img src="6.png" alt="image-20220719143043111"></p><p>仅训练10个epoch，acc就达到了0.977 ， 增大epoch应该还可以提高acc，但由于这里显卡不太行，所以只跑了10个epoch，后续可以考虑增大epoch，让网络达到收敛。</p><h2 id="4-预测图片"><a href="#4-预测图片" class="headerlink" title="4. 预测图片"></a>4. 预测图片</h2><p>打开文件系统，选择一张图片进行预测。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> torch<br><span class="hljs-keyword">from</span> PIL <span class="hljs-keyword">import</span> Image<br><span class="hljs-keyword">from</span> torchvision <span class="hljs-keyword">import</span> transforms<br><span class="hljs-keyword">import</span> matplotlib.pyplot <span class="hljs-keyword">as</span> plt<br><span class="hljs-keyword">import</span> json<br><span class="hljs-keyword">import</span> tkinter<br><span class="hljs-keyword">from</span> tkinter <span class="hljs-keyword">import</span> filedialog<br><span class="hljs-keyword">import</span> torchvision<br><br><span class="hljs-comment"># 预处理</span><br>data_transform = transforms.Compose(<br>    [transforms.Resize((<span class="hljs-number">224</span>, <span class="hljs-number">224</span>)),<br>     transforms.ToTensor(),<br>     transforms.Normalize((<span class="hljs-number">0.5</span>, <span class="hljs-number">0.5</span>, <span class="hljs-number">0.5</span>), (<span class="hljs-number">0.5</span>, <span class="hljs-number">0.5</span>, <span class="hljs-number">0.5</span>))])<br><br>window = tkinter.Tk()<br>window.withdraw()<br>img_file_path = filedialog.askopenfilename()  <span class="hljs-comment"># 获得选择好的文件</span><br><br><span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;Filepath:&#x27;</span>, img_file_path)<br><br><span class="hljs-comment"># load image</span><br>img = Image.<span class="hljs-built_in">open</span>(img_file_path)<br>plt.imshow(img)<br><span class="hljs-comment"># [N, C, H, W]</span><br>img = data_transform(img)<br><span class="hljs-comment"># expand batch dimension</span><br>img = torch.unsqueeze(img, dim=<span class="hljs-number">0</span>)<br><br>class_indict = <span class="hljs-literal">None</span><br><br><span class="hljs-comment"># read class_indict</span><br><span class="hljs-keyword">try</span>:<br>    json_file = <span class="hljs-built_in">open</span>(<span class="hljs-string">&#x27;./class_indices.json&#x27;</span>, <span class="hljs-string">&#x27;r&#x27;</span>, encoding=<span class="hljs-string">&quot;utf-8&quot;</span>)<br>    class_indict = json.load(json_file)<br><span class="hljs-keyword">except</span> Exception <span class="hljs-keyword">as</span> e:<br>    <span class="hljs-built_in">print</span>(e)<br>    exit(-<span class="hljs-number">1</span>)<br><br><span class="hljs-comment"># create model</span><br>model = torchvision.models.efficientnet_b1(pretrained=<span class="hljs-literal">True</span>)<br><span class="hljs-comment"># load model weights</span><br>model_weight_path = <span class="hljs-string">&quot;model.pth&quot;</span><br>model.load_state_dict(torch.load(model_weight_path))<br><br><span class="hljs-comment"># 关闭 Dropout</span><br>model.<span class="hljs-built_in">eval</span>()<br><span class="hljs-keyword">with</span> torch.no_grad():<br>    <span class="hljs-comment"># predict class</span><br>    output = torch.squeeze(model(img))  <span class="hljs-comment"># 将输出压缩，即压缩掉 batch 这个维度</span><br>    predict = torch.softmax(output, dim=<span class="hljs-number">0</span>)<br>    predict_cla = torch.argmax(predict).numpy()<br><br><span class="hljs-built_in">print</span>(class_indict[<span class="hljs-built_in">str</span>(predict_cla)], predict[predict_cla].item())<br>plt.show()<br><br></code></pre></td></tr></table></figure><p><img src="8.png" alt="image-20220719145611043"></p><p>预测结果为：<img src="9.png" alt="image-20220719145633757"></p><h2 id="5-总结"><a href="#5-总结" class="headerlink" title="5. 总结"></a>5. 总结</h2><p>这次用pytorch实现了一个简单的垃圾分类小demo，后续可以编写交互界面，增大训练集，进一步完善。</p>]]></content>
    
    
    <categories>
      
      <category>Python</category>
      
    </categories>
    
    
    <tags>
      
      <tag>原创</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>疫情微博情感识别</title>
    <link href="/2022/07/05/bert%E7%9A%84%E5%9F%BA%E6%9C%AC%E4%BD%BF%E7%94%A8/"/>
    <url>/2022/07/05/bert%E7%9A%84%E5%9F%BA%E6%9C%AC%E4%BD%BF%E7%94%A8/</url>
    
    <content type="html"><![CDATA[<h1 id="零基础入门NLP赛事"><a href="#零基础入门NLP赛事" class="headerlink" title="零基础入门NLP赛事"></a>零基础入门NLP赛事</h1><h2 id="1-报名比赛"><a href="#1-报名比赛" class="headerlink" title="1. 报名比赛"></a>1. 报名比赛</h2><h3 id="1-1链接：-http-challenge-xfyun-cn-topic-info-type-epidemic-weibo"><a href="#1-1链接：-http-challenge-xfyun-cn-topic-info-type-epidemic-weibo" class="headerlink" title="1.1链接： http://challenge.xfyun.cn/topic/info?type=epidemic-weibo"></a>1.1链接： <a href="http://challenge.xfyun.cn/topic/info?type=epidemic-weibo">http://challenge.xfyun.cn/topic/info?type=epidemic-weibo</a></h3><h3 id="1-2-赛事任务如图："><a href="#1-2-赛事任务如图：" class="headerlink" title="1.2 赛事任务如图："></a>1.2 赛事任务如图：</h3><p><img src="img.png" alt="img.png"></p><h3 id="1-3-评估指标为AUC"><a href="#1-3-评估指标为AUC" class="headerlink" title="1.3 评估指标为AUC"></a>1.3 评估指标为AUC</h3><h2 id="2-bert相关工具类"><a href="#2-bert相关工具类" class="headerlink" title="2.bert相关工具类"></a>2.bert相关工具类</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> torch<br><span class="hljs-keyword">import</span> torch.nn.functional <span class="hljs-keyword">as</span> F<br><span class="hljs-keyword">from</span> torch <span class="hljs-keyword">import</span> nn<br><span class="hljs-keyword">from</span> torch.utils.data <span class="hljs-keyword">import</span> Dataset, DataLoader<br><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> BertModel<br><br><br><span class="hljs-keyword">class</span> <span class="hljs-title class_">RoleDataset</span>(<span class="hljs-title class_ inherited__">Dataset</span>):<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self, texts, labels, tokenizer, max_len</span>):<br>        self.texts = texts<br>        self.labels = labels<br>        self.tokenizer = tokenizer  <span class="hljs-comment"># 分词</span><br>        self.max_len = max_len<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__len__</span>(<span class="hljs-params">self</span>):<br>        <span class="hljs-keyword">return</span> <span class="hljs-built_in">len</span>(self.texts)<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__getitem__</span>(<span class="hljs-params">self, item</span>):<br>        <span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-string">        item 为数据索引，迭代取第item条数据</span><br><span class="hljs-string">        &quot;&quot;&quot;</span><br>        text = <span class="hljs-built_in">str</span>(self.texts[item])<br>        label = self.labels[item]<br><br>        encoding = self.tokenizer.encode_plus(<br>            text,<br>            add_special_tokens=<span class="hljs-literal">True</span>,<br>            max_length=self.max_len,<br>            return_token_type_ids=<span class="hljs-literal">True</span>,<br>            pad_to_max_length=<span class="hljs-literal">True</span>,<br>            return_attention_mask=<span class="hljs-literal">True</span>,<br>            return_tensors=<span class="hljs-string">&#x27;pt&#x27;</span>,<br>            truncation=<span class="hljs-literal">True</span><br>        )<br><br>        sampler = &#123;<br>            <span class="hljs-string">&#x27;texts&#x27;</span>: text,<br>            <span class="hljs-string">&#x27;input_ids&#x27;</span>: encoding[<span class="hljs-string">&#x27;input_ids&#x27;</span>].flatten(),<br>            <span class="hljs-string">&#x27;attention_mask&#x27;</span>: encoding[<span class="hljs-string">&#x27;attention_mask&#x27;</span>].flatten(),<br>            <span class="hljs-string">&#x27;label&#x27;</span>: torch.tensor(label)<br>        &#125;<br><br>        <span class="hljs-keyword">return</span> sampler<br><br><br><span class="hljs-comment"># 创建数据集函数</span><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">create_data_loader</span>(<span class="hljs-params">df, tokenizer, max_len, batch_size</span>):<br>    ds = RoleDataset(<br>        texts=df[<span class="hljs-string">&#x27;text&#x27;</span>].values,<br>        labels=df[<span class="hljs-string">&#x27;label&#x27;</span>].values,<br>        tokenizer=tokenizer,<br>        max_len=max_len<br>    )<br><br>    <span class="hljs-keyword">return</span> DataLoader(<br>        ds,<br>        batch_size=batch_size,<br>    )<br><br><br><span class="hljs-keyword">class</span> <span class="hljs-title class_">AttentionHead</span>(nn.Module):<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self, in_features, hidden_dim</span>):<br>        <span class="hljs-built_in">super</span>().__init__()<br>        self.W = nn.Linear(in_features, hidden_dim)<br>        self.V = nn.Linear(hidden_dim, <span class="hljs-number">1</span>)<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">forward</span>(<span class="hljs-params">self, features</span>):<br>        att = torch.tanh(self.W(features))<br>        score = self.V(att)<br>        attention_weights = torch.softmax(score, dim=<span class="hljs-number">1</span>)<br>        context_vector = torch.<span class="hljs-built_in">sum</span>(attention_weights * features, dim=<span class="hljs-number">1</span>)<br>        <span class="hljs-keyword">return</span> context_vector<br><br><br><span class="hljs-comment"># fine-tuning</span><br><span class="hljs-keyword">class</span> <span class="hljs-title class_">EmotionClassifier</span>(nn.Module):<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self, n_classes, PRE_TRAINED_MODEL_NAME</span>):<br>        <span class="hljs-built_in">super</span>(EmotionClassifier, self).__init__()<br>        self.bert = BertModel.from_pretrained(PRE_TRAINED_MODEL_NAME)<br>        self.out = nn.Linear(self.bert.config.hidden_size * <span class="hljs-number">2</span>, n_classes)<br>        self.head = AttentionHead(self.bert.config.hidden_size, self.bert.config.hidden_size)<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">forward</span>(<span class="hljs-params">self, input_ids, attention_mask</span>):<br>        output = self.bert(<br>            input_ids=input_ids,<br>            attention_mask=attention_mask<br>        )<br>        last_hidden_state = output[<span class="hljs-number">0</span>]<br>        input_mask_expanded = attention_mask.unsqueeze(-<span class="hljs-number">1</span>).expand(last_hidden_state.size()).<span class="hljs-built_in">float</span>()<br>        sum_embeddings = torch.<span class="hljs-built_in">sum</span>(last_hidden_state * input_mask_expanded, <span class="hljs-number">1</span>)<br>        sum_mask = input_mask_expanded.<span class="hljs-built_in">sum</span>(<span class="hljs-number">1</span>)<br>        sum_mask = torch.clamp(sum_mask, <span class="hljs-built_in">min</span>=<span class="hljs-number">1e-9</span>)<br>        mean_embeddings = sum_embeddings / sum_mask<br>        head_out = self.head(output[<span class="hljs-number">0</span>])<br>        all_out = torch.cat([head_out, mean_embeddings], <span class="hljs-number">1</span>)<br>        logits = self.out(all_out)<br>        <span class="hljs-keyword">return</span> logits<br><br><br><span class="hljs-keyword">class</span> <span class="hljs-title class_">FGM</span>:<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self, model</span>):<br>        self.model = model<br>        self.backup = &#123;&#125;<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">attack</span>(<span class="hljs-params">self, epsilon=<span class="hljs-number">1.</span>, emb_name=<span class="hljs-string">&#x27;word_embeddings&#x27;</span></span>):<br>        <span class="hljs-comment"># emb_name这个参数要换成你模型中embedding的参数名</span><br>        <span class="hljs-keyword">for</span> name, param <span class="hljs-keyword">in</span> self.model.named_parameters():<br>            <span class="hljs-keyword">if</span> param.requires_grad <span class="hljs-keyword">and</span> emb_name <span class="hljs-keyword">in</span> name:<br>                self.backup[name] = param.data.clone()<br>                <span class="hljs-comment">#                 print(&#x27;grad: &#x27;, param.grad)</span><br>                norm = torch.norm(param.grad)<br>                <span class="hljs-keyword">if</span> norm != <span class="hljs-number">0</span>:<br>                    r_at = epsilon * param.grad / norm<br>                    param.data.add_(r_at)<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">restore</span>(<span class="hljs-params">self, emb_name=<span class="hljs-string">&#x27;word_embeddings&#x27;</span></span>):<br>        <span class="hljs-comment"># emb_name这个参数要换成你模型中embedding的参数名</span><br>        <span class="hljs-keyword">for</span> name, param <span class="hljs-keyword">in</span> self.model.named_parameters():<br>            <span class="hljs-keyword">if</span> param.requires_grad <span class="hljs-keyword">and</span> emb_name <span class="hljs-keyword">in</span> name:<br>                <span class="hljs-keyword">assert</span> name <span class="hljs-keyword">in</span> self.backup<br>                param.data = self.backup[name]<br>        self.backup = &#123;&#125;<br><br><br><span class="hljs-keyword">class</span> <span class="hljs-title class_">FocalLoss</span>(nn.Module):<br>    <span class="hljs-string">&quot;&quot;&quot;Multi-class Focal loss implementation&quot;&quot;&quot;</span><br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self, gamma=<span class="hljs-number">2</span>, weight=<span class="hljs-literal">None</span>, reduction=<span class="hljs-string">&#x27;mean&#x27;</span>, ignore_index=-<span class="hljs-number">100</span></span>):<br>        <span class="hljs-built_in">super</span>(FocalLoss, self).__init__()<br>        self.gamma = gamma<br>        self.weight = weight<br>        self.ignore_index = ignore_index<br>        self.reduction = reduction<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">forward</span>(<span class="hljs-params">self, inputs, target</span>):<br>        <span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-string">        input: [N, C]</span><br><span class="hljs-string">        target: [N, ]</span><br><span class="hljs-string">        &quot;&quot;&quot;</span><br>        target = target.long()<br>        log_pt = torch.log_softmax(inputs, dim=<span class="hljs-number">1</span>)<br>        pt = torch.exp(log_pt)<br>        log_pt = (<span class="hljs-number">1</span> - pt) ** self.gamma * log_pt<br>        loss = torch.nn.functional.nll_loss(log_pt, target, self.weight, reduction=self.reduction,<br>                                            ignore_index=self.ignore_index)<br>        <span class="hljs-keyword">return</span> loss<br></code></pre></td></tr></table></figure><h2 id="3-比赛baseline"><a href="#3-比赛baseline" class="headerlink" title="3. 比赛baseline"></a>3. 比赛baseline</h2><figure class="highlight routeros"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br></pre></td><td class="code"><pre><code class="hljs routeros">import pandas as pd<br>import torch<br><span class="hljs-keyword">from</span> tools import create_data_loader, EmotionClassifier, FocalLoss, FGM<br>import torch.nn as nn<br>import warnings<br>import glob<br>import numpy as np<br><span class="hljs-keyword">from</span> transformers import BertTokenizer, AdamW, get_linear_schedule_with_warmup<br>import os<br><span class="hljs-keyword">from</span> sklearn.model_selection import StratifiedKFold<br><span class="hljs-keyword">from</span> tqdm import tqdm<br><br>os.environ[<span class="hljs-string">&#x27;CUDA_LAUNCH_BLOCKING&#x27;</span>] = <span class="hljs-string">&#x27;1&#x27;</span><br>warnings.filterwarnings(<span class="hljs-string">&#x27;ignore&#x27;</span>)<br><br>df = pd.read_csv(<span class="hljs-string">&#x27;../data/clean_data.csv&#x27;</span>)<br>train = df[0:60000]<br>train[<span class="hljs-string">&#x27;label&#x27;</span>] = train[<span class="hljs-string">&#x27;label&#x27;</span>].astype(<span class="hljs-string">&#x27;long&#x27;</span>)<br>test = df[-10000:]<br><br>RANDOM_SEED = 42<br>np.random.seed(RANDOM_SEED)<br><span class="hljs-comment"># torch.manual_seed(RANDOM_SEED)</span><br><br>device = torch.device(<span class="hljs-string">&quot;cuda:0&quot;</span> <span class="hljs-keyword">if</span> torch.cuda.is_available() <span class="hljs-keyword">else</span> <span class="hljs-string">&quot;cpu&quot;</span>)<br><span class="hljs-comment"># device = torch.device(&#x27;cpu&#x27;)</span><br><span class="hljs-built_in">print</span>(torch.cuda.is_available())<br><br>PRE_TRAINED_MODEL_NAME = <span class="hljs-string">&#x27;../pre_train_model/roberta&#x27;</span><br>tokenizer = BertTokenizer.from_pretrained(PRE_TRAINED_MODEL_NAME)<br><br>skf = StratifiedKFold(<span class="hljs-attribute">n_splits</span>=5, <span class="hljs-attribute">random_state</span>=2022, <span class="hljs-attribute">shuffle</span>=<span class="hljs-literal">True</span>)<br>fold_idx = 1<br><br>EPOCHS = 1  # 训练轮数<br>BATCH_SIZE = 1<br>MAX_LEN = 20<br>LR = 3e-5<br><br><span class="hljs-keyword">for</span> train_idx, val_idx <span class="hljs-keyword">in</span> skf.split(train[<span class="hljs-string">&#x27;label&#x27;</span>], train[<span class="hljs-string">&#x27;label&#x27;</span>]):<br>    train_data_loader = create_data_loader(train.iloc[train_idx], tokenizer, MAX_LEN, BATCH_SIZE)<br>    val_data_loader = create_data_loader(train.iloc[val_idx], tokenizer, MAX_LEN, BATCH_SIZE)<br>    model = EmotionClassifier(2, PRE_TRAINED_MODEL_NAME)<br>    model.<span class="hljs-keyword">to</span>(device)<br><br>    optimizer = AdamW(model.parameters(), <span class="hljs-attribute">lr</span>=LR, <span class="hljs-attribute">weight_decay</span>=0.01)<br>    total_steps = len(train_data_loader) * EPOCHS<br><br>   <span class="hljs-built_in"> scheduler </span>= get_linear_schedule_with_warmup(<br>        optimizer,<br>        <span class="hljs-attribute">num_warmup_steps</span>=0.1,<br>        <span class="hljs-attribute">num_training_steps</span>=total_steps<br>    )<br><br>    loss_fn = FocalLoss().<span class="hljs-keyword">to</span>(device)<br><br>    best_acc = 0<br>    fgm = FGM(model)<br>    <span class="hljs-keyword">for</span> epoch <span class="hljs-keyword">in</span> range(EPOCHS):<br>        # 训练<br>        model = model.train()<br>        j = 1<br>        <span class="hljs-keyword">for</span> sample <span class="hljs-keyword">in</span> tqdm(train_data_loader):<br>            input_ids = sample[<span class="hljs-string">&quot;input_ids&quot;</span>].<span class="hljs-keyword">to</span>(device)<br>            attention_mask = sample[<span class="hljs-string">&quot;attention_mask&quot;</span>].<span class="hljs-keyword">to</span>(device)<br>            label = sample[<span class="hljs-string">&#x27;label&#x27;</span>].<span class="hljs-keyword">to</span>(device)<br>            logits = model(<br>                <span class="hljs-attribute">input_ids</span>=input_ids,<br>                <span class="hljs-attribute">attention_mask</span>=attention_mask<br>            )<br>            loss = loss_fn(logits, label)<br><br>            <span class="hljs-keyword">if</span> j % 50 == 0:<br>                <span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;Train %d: loss:%f&#x27;</span> % (j, loss.detach().cpu().numpy()))<br>            loss.backward()<br>            nn.utils.clip_grad_norm_(model.parameters(), <span class="hljs-attribute">max_norm</span>=1.0)<br>            # 对抗训练<br>            fgm.attack()  # 在embedding上添加对抗扰动<br>            loss_adv = model(<br>                <span class="hljs-attribute">input_ids</span>=input_ids,<br>                <span class="hljs-attribute">attention_mask</span>=attention_mask<br>            )<br>            loss_adv2 = loss_fn(loss_adv, label)<br>            loss_adv2.backward()  # 反向传播，并在正常的grad基础上，累加对抗训练的梯度<br>            fgm.restore()  # 恢复embedding参数<br><br>            optimizer.<span class="hljs-keyword">step</span>()<br>            scheduler.<span class="hljs-keyword">step</span>()<br>            optimizer.zero_grad()<br><br>            # 验证<br>            <span class="hljs-keyword">if</span> (j % 100 == 0) <span class="hljs-keyword">or</span> (j % len(train_data_loader) == 0):<br>                all_pred, all_label = [], []<br>                with torch.no_grad():<br>                    model = model.eval()<br>                    <span class="hljs-keyword">for</span> sampler <span class="hljs-keyword">in</span> val_data_loader:<br>                        input_ids = sampler[<span class="hljs-string">&quot;input_ids&quot;</span>].<span class="hljs-keyword">to</span>(device)<br>                        attention_mask = sampler[<span class="hljs-string">&quot;attention_mask&quot;</span>].<span class="hljs-keyword">to</span>(device)<br>                        label = sampler[<span class="hljs-string">&#x27;label&#x27;</span>].<span class="hljs-keyword">to</span>(device)<br>                        logits = model(<br>                            <span class="hljs-attribute">input_ids</span>=input_ids,<br>                            <span class="hljs-attribute">attention_mask</span>=attention_mask<br>                        )<br>                        all_pred.extend(logits.argmax(-1).detach().cpu().numpy())<br>                        all_label.extend(label.detach().cpu().numpy())<br>                    model = model.train()<br>                    acc = (np.array(all_label) == np.array(all_pred)).astype(np.float32).mean()<br>                    <span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;Val acc %.5f&#x27;</span> % acc)<br>                    <span class="hljs-keyword">if</span> acc &gt; best_acc:<br>                        best_acc = acc<br>                        torch.save(model.state_dict(), <span class="hljs-string">&#x27;../models//&#x27;</span> + str(fold_idx) + <span class="hljs-string">&#x27;.pth&#x27;</span>)<br>            j += 1<br>    fold_idx += 1<br>    <br><span class="hljs-comment"># 预测</span><br>test[<span class="hljs-string">&#x27;label&#x27;</span>] = 0<br>test_data_loader = create_data_loader(test, tokenizer, MAX_LEN, BATCH_SIZE)<br><br>test_pred_tta = None<br><span class="hljs-keyword">for</span> path <span class="hljs-keyword">in</span> glob.glob(<span class="hljs-string">&#x27;../models//*.pth&#x27;</span>):<br>    # 加载模型<br>    <span class="hljs-built_in">print</span>(path)<br>    model = EmotionClassifier(2, PRE_TRAINED_MODEL_NAME)<br>    model.load_state_dict(torch.load(path, <span class="hljs-attribute">map_location</span>=<span class="hljs-string">&quot;cpu&quot;</span>))<br>    model.<span class="hljs-keyword">to</span>(device)<br><br>    test_pred = []<br>    <span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;load model finished&quot;</span>)<br>    with torch.no_grad():<br>        model.eval()<br>        <span class="hljs-keyword">for</span> sample <span class="hljs-keyword">in</span> test_data_loader:<br>            input_ids = sample[<span class="hljs-string">&quot;input_ids&quot;</span>].<span class="hljs-keyword">to</span>(device)<br>            attention_mask = sample[<span class="hljs-string">&quot;attention_mask&quot;</span>].<span class="hljs-keyword">to</span>(device)<br>            logits = model(<br>                <span class="hljs-attribute">input_ids</span>=input_ids,<br>                <span class="hljs-attribute">attention_mask</span>=attention_mask<br>            )<br>            test_pred.append(logits.detach().cpu().numpy())<br><br>    <span class="hljs-keyword">if</span> test_pred_tta is None:<br>        test_pred_tta = np.vstack(test_pred)<br>    <span class="hljs-keyword">else</span>:<br>        test_pred_tta += np.vstack(test_pred)<br><br><span class="hljs-comment"># 生成结果</span><br>test[<span class="hljs-string">&#x27;label&#x27;</span>] = test_pred_tta.argmax(1)<br></code></pre></td></tr></table></figure>]]></content>
    
    
    <categories>
      
      <category>NLP</category>
      
    </categories>
    
    
    <tags>
      
      <tag>原创</tag>
      
    </tags>
    
  </entry>
  
  
  
  
</search>
